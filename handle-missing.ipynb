{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обрабатывать пропущенные временные шаги в задачах прогнозирования последовательности с помощью Python\n",
    "https://machinelearningmastery.com/handle-missing-timesteps-sequence-prediction-problems-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Демонстрация проблемы последовательности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[       nan 0.40531374] => nan\n",
      "[0.40531374 0.02901154] => 0.4053137372794514\n",
      "[0.02901154 0.80863035] => 0.029011540917747936\n",
      "[0.80863035 0.83176467] => 0.8086303509380647\n",
      "[0.83176467 0.45082904] => 0.8317646709920837\n",
      "[0.45082904 0.74522144] => 0.45082903625450355\n",
      "[0.74522144 0.64456485] => 0.7452214369304784\n",
      "[0.64456485 0.59238968] => 0.6445648508186052\n",
      "[0.59238968 0.82743259] => 0.5923896844090002\n",
      "[0.82743259 0.37163253] => 0.8274325915552974\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "from numpy import array\n",
    "from pandas import concat\n",
    "from pandas import DataFrame\n",
    "\n",
    "# generate a sequence of random values\n",
    "def generate_sequence(n_timesteps):\n",
    "\treturn [random() for _ in range(n_timesteps)]\n",
    "\n",
    "# generate data for the lstm\n",
    "def generate_data(n_timesteps):\n",
    "\t# generate sequence\n",
    "\tsequence = generate_sequence(n_timesteps)\n",
    "\tsequence = array(sequence)\n",
    "\t# create lag\n",
    "\tdf = DataFrame(sequence)\n",
    "\tdf = concat([df.shift(1), df], axis=1)\n",
    "\tvalues = df.values\n",
    "\t# specify input and output data\n",
    "\tX, y = values, values[:, 0]\n",
    "\treturn X, y\n",
    "\n",
    "# generate sequence\n",
    "n_timesteps = 10\n",
    "X, y = generate_data(n_timesteps)\n",
    "# print sequence\n",
    "for i in range(n_timesteps):\n",
    "\tprint(X[i], '=>', y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Удалить отсутствующие данные последовательности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.60703897 0.46145566] => 0.6070389652880486\n",
      "[0.46145566 0.66840817] => 0.46145565525111154\n",
      "[0.66840817 0.06200417] => 0.668408166535919\n",
      "[0.06200417 0.63484706] => 0.06200416849407375\n",
      "[0.63484706 0.30086317] => 0.634847059222349\n",
      "[0.30086317 0.98016782] => 0.3008631709157785\n",
      "[0.98016782 0.9905226 ] => 0.9801678207477256\n",
      "[0.9905226  0.07593411] => 0.9905226030519235\n",
      "[0.07593411 0.47941179] => 0.07593410985452864\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "from numpy import array\n",
    "from pandas import concat\n",
    "from pandas import DataFrame\n",
    "\n",
    "# generate a sequence of random values\n",
    "def generate_sequence(n_timesteps):\n",
    "\treturn [random() for _ in range(n_timesteps)]\n",
    "\n",
    "# generate data for the lstm\n",
    "def generate_data(n_timesteps):\n",
    "\t# generate sequence\n",
    "\tsequence = generate_sequence(n_timesteps)\n",
    "\tsequence = array(sequence)\n",
    "\t# create lag\n",
    "\tdf = DataFrame(sequence)\n",
    "\tdf = concat([df.shift(1), df], axis=1)\n",
    "\t# remove rows with missing values\n",
    "\tdf.dropna(inplace=True)\n",
    "\tvalues = df.values\n",
    "\t# specify input and output data\n",
    "\tX, y = values, values[:, 0]\n",
    "\treturn X, y\n",
    "\n",
    "# generate sequence\n",
    "n_timesteps = 10\n",
    "X, y = generate_data(n_timesteps)\n",
    "# print sequence\n",
    "for i in range(len(X)):\n",
    "\tprint(X[i], '=>', y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заменить отсутствующие данные последовательности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.          0.35551706] => 0.35551705812576406\n",
      "[0.35551706 0.08252663] => 0.08252663045139808\n",
      "[0.08252663 0.4242715 ] => 0.424271501747155\n",
      "[0.4242715 0.7377109] => 0.73771089867304\n",
      "[0.7377109  0.52284893] => 0.5228489302195808\n",
      "[0.52284893 0.36435033] => 0.36435033403852546\n",
      "[0.36435033 0.02632615] => 0.02632615153395379\n",
      "[0.02632615 0.5680358 ] => 0.5680357977683799\n",
      "[0.5680358  0.85151198] => 0.851511981805571\n",
      "[0.85151198 0.49000946] => 0.49000946238349363\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "from numpy import array\n",
    "from pandas import concat\n",
    "from pandas import DataFrame\n",
    "\n",
    "# generate a sequence of random values\n",
    "def generate_sequence(n_timesteps):\n",
    "\treturn [random() for _ in range(n_timesteps)]\n",
    "\n",
    "# generate data for the lstm\n",
    "def generate_data(n_timesteps):\n",
    "\t# generate sequence\n",
    "\tsequence = generate_sequence(n_timesteps)\n",
    "\tsequence = array(sequence)\n",
    "\t# create lag\n",
    "\tdf = DataFrame(sequence)\n",
    "\tdf = concat([df.shift(1), df], axis=1)\n",
    "\t# replace missing values with -1\n",
    "\tdf.fillna(-1, inplace=True)\n",
    "\tvalues = df.values\n",
    "\t# specify input and output data\n",
    "\tX, y = values, values[:, 1]\n",
    "\treturn X, y\n",
    "\n",
    "# generate sequence\n",
    "n_timesteps = 10\n",
    "X, y = generate_data(n_timesteps)\n",
    "# print sequence\n",
    "for i in range(len(X)):\n",
    "\tprint(X[i], '=>', y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение с отсутствующими значениями последовательности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изучение недостающих ценностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-26 00:17:27.091366: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-10-26 00:17:27.091681: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-26 00:17:27.093497: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2021-10-26 00:17:27.449735: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-10-26 00:17:27.469583: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2594090000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 3s - loss: 0.1975\n",
      "10/10 - 0s - loss: 0.2079\n",
      "10/10 - 0s - loss: 0.2507\n",
      "10/10 - 0s - loss: 0.1923\n",
      "10/10 - 0s - loss: 0.2327\n",
      "10/10 - 0s - loss: 0.1874\n",
      "10/10 - 0s - loss: 0.1816\n",
      "10/10 - 0s - loss: 0.0817\n",
      "10/10 - 0s - loss: 0.0870\n",
      "10/10 - 0s - loss: 0.1186\n",
      "10/10 - 0s - loss: 0.0696\n",
      "10/10 - 0s - loss: 0.1323\n",
      "10/10 - 0s - loss: 0.1436\n",
      "10/10 - 0s - loss: 0.0710\n",
      "10/10 - 0s - loss: 0.0416\n",
      "10/10 - 0s - loss: 0.0975\n",
      "10/10 - 0s - loss: 0.0985\n",
      "10/10 - 0s - loss: 0.0925\n",
      "10/10 - 0s - loss: 0.0720\n",
      "10/10 - 0s - loss: 0.0416\n",
      "10/10 - 0s - loss: 0.0370\n",
      "10/10 - 0s - loss: 0.0478\n",
      "10/10 - 0s - loss: 0.0971\n",
      "10/10 - 0s - loss: 0.0362\n",
      "10/10 - 0s - loss: 0.0438\n",
      "10/10 - 0s - loss: 0.0542\n",
      "10/10 - 0s - loss: 0.0569\n",
      "10/10 - 0s - loss: 0.0569\n",
      "10/10 - 0s - loss: 0.0531\n",
      "10/10 - 0s - loss: 0.0758\n",
      "10/10 - 0s - loss: 0.0895\n",
      "10/10 - 0s - loss: 0.0316\n",
      "10/10 - 0s - loss: 0.0479\n",
      "10/10 - 0s - loss: 0.0496\n",
      "10/10 - 0s - loss: 0.0691\n",
      "10/10 - 0s - loss: 0.0345\n",
      "10/10 - 0s - loss: 0.0647\n",
      "10/10 - 0s - loss: 0.0350\n",
      "10/10 - 0s - loss: 0.0389\n",
      "10/10 - 0s - loss: 0.0256\n",
      "10/10 - 0s - loss: 0.0213\n",
      "10/10 - 0s - loss: 0.0745\n",
      "10/10 - 0s - loss: 0.0276\n",
      "10/10 - 0s - loss: 0.0604\n",
      "10/10 - 0s - loss: 0.0461\n",
      "10/10 - 0s - loss: 0.0431\n",
      "10/10 - 0s - loss: 0.0627\n",
      "10/10 - 0s - loss: 0.0569\n",
      "10/10 - 0s - loss: 0.0562\n",
      "10/10 - 0s - loss: 0.0929\n",
      "10/10 - 0s - loss: 0.0221\n",
      "10/10 - 0s - loss: 0.0660\n",
      "10/10 - 0s - loss: 0.0314\n",
      "10/10 - 0s - loss: 0.0376\n",
      "10/10 - 0s - loss: 0.0494\n",
      "10/10 - 0s - loss: 0.0347\n",
      "10/10 - 0s - loss: 0.0272\n",
      "10/10 - 0s - loss: 0.0382\n",
      "10/10 - 0s - loss: 0.0414\n",
      "10/10 - 0s - loss: 0.0298\n",
      "10/10 - 0s - loss: 0.0635\n",
      "10/10 - 0s - loss: 0.0606\n",
      "10/10 - 0s - loss: 0.0360\n",
      "10/10 - 0s - loss: 0.0532\n",
      "10/10 - 0s - loss: 0.0270\n",
      "10/10 - 0s - loss: 0.0241\n",
      "10/10 - 0s - loss: 0.0438\n",
      "10/10 - 0s - loss: 0.0131\n",
      "10/10 - 0s - loss: 0.0434\n",
      "10/10 - 0s - loss: 0.0166\n",
      "10/10 - 0s - loss: 0.0380\n",
      "10/10 - 0s - loss: 0.0453\n",
      "10/10 - 0s - loss: 0.0342\n",
      "10/10 - 0s - loss: 0.0250\n",
      "10/10 - 0s - loss: 0.0149\n",
      "10/10 - 0s - loss: 0.0309\n",
      "10/10 - 0s - loss: 0.0210\n",
      "10/10 - 0s - loss: 0.0410\n",
      "10/10 - 0s - loss: 0.0209\n",
      "10/10 - 0s - loss: 0.0289\n",
      "10/10 - 0s - loss: 0.0425\n",
      "10/10 - 0s - loss: 0.0333\n",
      "10/10 - 0s - loss: 0.0281\n",
      "10/10 - 0s - loss: 0.0329\n",
      "10/10 - 0s - loss: 0.0062\n",
      "10/10 - 0s - loss: 0.0288\n",
      "10/10 - 0s - loss: 0.0476\n",
      "10/10 - 0s - loss: 0.0165\n",
      "10/10 - 0s - loss: 0.0226\n",
      "10/10 - 0s - loss: 0.0158\n",
      "10/10 - 0s - loss: 0.0300\n",
      "10/10 - 0s - loss: 0.0104\n",
      "10/10 - 0s - loss: 0.0118\n",
      "10/10 - 0s - loss: 0.0227\n",
      "10/10 - 0s - loss: 0.0316\n",
      "10/10 - 0s - loss: 0.0150\n",
      "10/10 - 0s - loss: 0.0181\n",
      "10/10 - 0s - loss: 0.0247\n",
      "10/10 - 0s - loss: 0.0098\n",
      "10/10 - 0s - loss: 0.0278\n",
      "10/10 - 0s - loss: 0.0130\n",
      "10/10 - 0s - loss: 0.0201\n",
      "10/10 - 0s - loss: 0.0142\n",
      "10/10 - 0s - loss: 0.0171\n",
      "10/10 - 0s - loss: 0.0200\n",
      "10/10 - 0s - loss: 0.0125\n",
      "10/10 - 0s - loss: 0.0207\n",
      "10/10 - 0s - loss: 0.0223\n",
      "10/10 - 0s - loss: 0.0149\n",
      "10/10 - 0s - loss: 0.0158\n",
      "10/10 - 0s - loss: 0.0161\n",
      "10/10 - 0s - loss: 0.0055\n",
      "10/10 - 0s - loss: 0.0104\n",
      "10/10 - 0s - loss: 0.0071\n",
      "10/10 - 0s - loss: 0.0134\n",
      "10/10 - 0s - loss: 0.0085\n",
      "10/10 - 0s - loss: 0.0111\n",
      "10/10 - 0s - loss: 0.0040\n",
      "10/10 - 0s - loss: 0.0116\n",
      "10/10 - 0s - loss: 0.0041\n",
      "10/10 - 0s - loss: 0.0035\n",
      "10/10 - 0s - loss: 0.0113\n",
      "10/10 - 0s - loss: 0.0171\n",
      "10/10 - 0s - loss: 0.0064\n",
      "10/10 - 0s - loss: 0.0101\n",
      "10/10 - 0s - loss: 0.0111\n",
      "10/10 - 0s - loss: 0.0084\n",
      "10/10 - 0s - loss: 0.0077\n",
      "10/10 - 0s - loss: 0.0044\n",
      "10/10 - 0s - loss: 0.0121\n",
      "10/10 - 0s - loss: 0.0072\n",
      "10/10 - 0s - loss: 0.0059\n",
      "10/10 - 0s - loss: 0.0073\n",
      "10/10 - 0s - loss: 0.0031\n",
      "10/10 - 0s - loss: 0.0103\n",
      "10/10 - 0s - loss: 0.0143\n",
      "10/10 - 0s - loss: 0.0066\n",
      "10/10 - 0s - loss: 0.0039\n",
      "10/10 - 0s - loss: 0.0070\n",
      "10/10 - 0s - loss: 0.0031\n",
      "10/10 - 0s - loss: 0.0033\n",
      "10/10 - 0s - loss: 0.0033\n",
      "10/10 - 0s - loss: 0.0037\n",
      "10/10 - 0s - loss: 0.0029\n",
      "10/10 - 0s - loss: 0.0044\n",
      "10/10 - 0s - loss: 0.0097\n",
      "10/10 - 0s - loss: 0.0065\n",
      "10/10 - 0s - loss: 0.0012\n",
      "10/10 - 0s - loss: 0.0017\n",
      "10/10 - 0s - loss: 0.0032\n",
      "10/10 - 0s - loss: 0.0094\n",
      "10/10 - 0s - loss: 0.0056\n",
      "10/10 - 0s - loss: 0.0020\n",
      "10/10 - 0s - loss: 0.0044\n",
      "10/10 - 0s - loss: 0.0016\n",
      "10/10 - 0s - loss: 0.0028\n",
      "10/10 - 0s - loss: 0.0040\n",
      "10/10 - 0s - loss: 0.0015\n",
      "10/10 - 0s - loss: 0.0036\n",
      "10/10 - 0s - loss: 5.9861e-04\n",
      "10/10 - 0s - loss: 0.0064\n",
      "10/10 - 0s - loss: 0.0040\n",
      "10/10 - 0s - loss: 0.0057\n",
      "10/10 - 0s - loss: 0.0054\n",
      "10/10 - 0s - loss: 0.0031\n",
      "10/10 - 0s - loss: 0.0040\n",
      "10/10 - 0s - loss: 0.0026\n",
      "10/10 - 0s - loss: 0.0108\n",
      "10/10 - 0s - loss: 0.0022\n",
      "10/10 - 0s - loss: 0.0037\n",
      "10/10 - 0s - loss: 0.0029\n",
      "10/10 - 0s - loss: 0.0016\n",
      "10/10 - 0s - loss: 0.0049\n",
      "10/10 - 0s - loss: 0.0045\n",
      "10/10 - 0s - loss: 0.0017\n",
      "10/10 - 0s - loss: 0.0056\n",
      "10/10 - 0s - loss: 0.0048\n",
      "10/10 - 0s - loss: 0.0031\n",
      "10/10 - 0s - loss: 0.0040\n",
      "10/10 - 0s - loss: 0.0030\n",
      "10/10 - 0s - loss: 0.0027\n",
      "10/10 - 0s - loss: 0.0016\n",
      "10/10 - 0s - loss: 0.0036\n",
      "10/10 - 0s - loss: 0.0038\n",
      "10/10 - 0s - loss: 0.0046\n",
      "10/10 - 0s - loss: 0.0023\n",
      "10/10 - 0s - loss: 0.0024\n",
      "10/10 - 0s - loss: 0.0017\n",
      "10/10 - 0s - loss: 0.0014\n",
      "10/10 - 0s - loss: 0.0025\n",
      "10/10 - 0s - loss: 0.0065\n",
      "10/10 - 0s - loss: 0.0017\n",
      "10/10 - 0s - loss: 0.0031\n",
      "10/10 - 0s - loss: 0.0016\n",
      "10/10 - 0s - loss: 0.0013\n",
      "10/10 - 0s - loss: 0.0030\n",
      "10/10 - 0s - loss: 5.3907e-04\n",
      "10/10 - 0s - loss: 0.0030\n",
      "10/10 - 0s - loss: 4.1856e-04\n",
      "10/10 - 0s - loss: 0.0013\n",
      "10/10 - 0s - loss: 0.0023\n",
      "10/10 - 0s - loss: 5.1306e-04\n",
      "10/10 - 0s - loss: 0.0021\n",
      "10/10 - 0s - loss: 7.8438e-04\n",
      "10/10 - 0s - loss: 0.0019\n",
      "10/10 - 0s - loss: 0.0032\n",
      "10/10 - 0s - loss: 0.0022\n",
      "10/10 - 0s - loss: 0.0041\n",
      "10/10 - 0s - loss: 0.0017\n",
      "10/10 - 0s - loss: 0.0011\n",
      "10/10 - 0s - loss: 0.0015\n",
      "10/10 - 0s - loss: 0.0024\n",
      "10/10 - 0s - loss: 0.0021\n",
      "10/10 - 0s - loss: 0.0015\n",
      "10/10 - 0s - loss: 0.0011\n",
      "10/10 - 0s - loss: 7.3402e-04\n",
      "10/10 - 0s - loss: 0.0014\n",
      "10/10 - 0s - loss: 0.0019\n",
      "10/10 - 0s - loss: 4.0609e-04\n",
      "10/10 - 0s - loss: 6.5354e-04\n",
      "10/10 - 0s - loss: 0.0027\n",
      "10/10 - 0s - loss: 0.0012\n",
      "10/10 - 0s - loss: 0.0011\n",
      "10/10 - 0s - loss: 5.6421e-04\n",
      "10/10 - 0s - loss: 7.2594e-04\n",
      "10/10 - 0s - loss: 6.1302e-04\n",
      "10/10 - 0s - loss: 0.0018\n",
      "10/10 - 0s - loss: 0.0011\n",
      "10/10 - 0s - loss: 7.7471e-04\n",
      "10/10 - 0s - loss: 0.0021\n",
      "10/10 - 0s - loss: 2.2900e-04\n",
      "10/10 - 0s - loss: 5.2661e-04\n",
      "10/10 - 0s - loss: 0.0015\n",
      "10/10 - 0s - loss: 5.0180e-04\n",
      "10/10 - 0s - loss: 2.0676e-04\n",
      "10/10 - 0s - loss: 8.6946e-04\n",
      "10/10 - 0s - loss: 2.8404e-04\n",
      "10/10 - 0s - loss: 1.6432e-04\n",
      "10/10 - 0s - loss: 0.0012\n",
      "10/10 - 0s - loss: 2.7853e-04\n",
      "10/10 - 0s - loss: 0.0012\n",
      "10/10 - 0s - loss: 6.0392e-04\n",
      "10/10 - 0s - loss: 8.9500e-04\n",
      "10/10 - 0s - loss: 3.2106e-04\n",
      "10/10 - 0s - loss: 3.0642e-04\n",
      "10/10 - 0s - loss: 0.0011\n",
      "10/10 - 0s - loss: 6.3807e-04\n",
      "10/10 - 0s - loss: 2.4596e-04\n",
      "10/10 - 0s - loss: 0.0013\n",
      "10/10 - 0s - loss: 6.5432e-04\n",
      "10/10 - 0s - loss: 3.2135e-04\n",
      "10/10 - 0s - loss: 5.1250e-04\n",
      "10/10 - 0s - loss: 6.3835e-04\n",
      "10/10 - 0s - loss: 7.0978e-04\n",
      "10/10 - 0s - loss: 0.0015\n",
      "10/10 - 0s - loss: 2.2607e-04\n",
      "10/10 - 0s - loss: 5.4130e-04\n",
      "10/10 - 0s - loss: 4.4486e-04\n",
      "10/10 - 0s - loss: 0.0014\n",
      "10/10 - 0s - loss: 6.0022e-04\n",
      "10/10 - 0s - loss: 5.3607e-04\n",
      "10/10 - 0s - loss: 3.9211e-04\n",
      "10/10 - 0s - loss: 6.3930e-04\n",
      "10/10 - 0s - loss: 6.6566e-04\n",
      "10/10 - 0s - loss: 0.0010\n",
      "10/10 - 0s - loss: 1.6491e-04\n",
      "10/10 - 0s - loss: 3.6498e-04\n",
      "10/10 - 0s - loss: 9.5404e-04\n",
      "10/10 - 0s - loss: 0.0010\n",
      "10/10 - 0s - loss: 7.4418e-04\n",
      "10/10 - 0s - loss: 1.4145e-04\n",
      "10/10 - 0s - loss: 6.3530e-04\n",
      "10/10 - 0s - loss: 4.0750e-04\n",
      "10/10 - 0s - loss: 6.1270e-04\n",
      "10/10 - 0s - loss: 3.2641e-04\n",
      "10/10 - 0s - loss: 2.1430e-04\n",
      "10/10 - 0s - loss: 2.2677e-04\n",
      "10/10 - 0s - loss: 6.6233e-05\n",
      "10/10 - 0s - loss: 4.7892e-04\n",
      "10/10 - 0s - loss: 4.3958e-04\n",
      "10/10 - 0s - loss: 3.5047e-04\n",
      "10/10 - 0s - loss: 2.9053e-04\n",
      "10/10 - 0s - loss: 0.0011\n",
      "10/10 - 0s - loss: 2.8546e-04\n",
      "10/10 - 0s - loss: 2.1688e-04\n",
      "10/10 - 0s - loss: 4.9815e-05\n",
      "10/10 - 0s - loss: 8.2599e-04\n",
      "10/10 - 0s - loss: 3.1826e-04\n",
      "10/10 - 0s - loss: 3.0433e-04\n",
      "10/10 - 0s - loss: 5.7168e-05\n",
      "10/10 - 0s - loss: 2.7988e-04\n",
      "10/10 - 0s - loss: 8.8692e-04\n",
      "10/10 - 0s - loss: 1.5200e-04\n",
      "10/10 - 0s - loss: 2.7031e-04\n",
      "10/10 - 0s - loss: 5.1430e-04\n",
      "10/10 - 0s - loss: 5.7975e-04\n",
      "10/10 - 0s - loss: 3.4305e-04\n",
      "10/10 - 0s - loss: 3.7293e-04\n",
      "10/10 - 0s - loss: 7.0189e-04\n",
      "10/10 - 0s - loss: 4.8831e-04\n",
      "10/10 - 0s - loss: 2.2545e-04\n",
      "10/10 - 0s - loss: 3.5460e-04\n",
      "10/10 - 0s - loss: 3.2255e-04\n",
      "10/10 - 0s - loss: 6.2193e-04\n",
      "10/10 - 0s - loss: 1.2469e-04\n",
      "10/10 - 0s - loss: 4.6237e-04\n",
      "10/10 - 0s - loss: 4.0700e-04\n",
      "10/10 - 0s - loss: 4.0632e-04\n",
      "10/10 - 0s - loss: 2.3437e-04\n",
      "10/10 - 0s - loss: 3.6489e-05\n",
      "10/10 - 0s - loss: 1.7273e-04\n",
      "10/10 - 0s - loss: 1.3163e-04\n",
      "10/10 - 0s - loss: 6.4063e-04\n",
      "10/10 - 0s - loss: 6.3491e-04\n",
      "10/10 - 0s - loss: 2.3056e-04\n",
      "10/10 - 0s - loss: 2.0527e-04\n",
      "10/10 - 0s - loss: 2.1135e-04\n",
      "10/10 - 0s - loss: 4.8144e-04\n",
      "10/10 - 0s - loss: 4.0035e-04\n",
      "10/10 - 0s - loss: 5.8419e-04\n",
      "10/10 - 0s - loss: 3.5171e-04\n",
      "10/10 - 0s - loss: 1.8885e-04\n",
      "10/10 - 0s - loss: 6.6153e-04\n",
      "10/10 - 0s - loss: 4.2201e-04\n",
      "10/10 - 0s - loss: 3.5877e-04\n",
      "10/10 - 0s - loss: 3.0348e-04\n",
      "10/10 - 0s - loss: 1.7892e-04\n",
      "10/10 - 0s - loss: 3.5684e-04\n",
      "10/10 - 0s - loss: 7.8879e-05\n",
      "10/10 - 0s - loss: 5.2071e-04\n",
      "10/10 - 0s - loss: 1.4042e-04\n",
      "10/10 - 0s - loss: 2.9923e-04\n",
      "10/10 - 0s - loss: 3.2774e-04\n",
      "10/10 - 0s - loss: 2.1585e-04\n",
      "10/10 - 0s - loss: 2.6950e-04\n",
      "10/10 - 0s - loss: 1.4016e-04\n",
      "10/10 - 0s - loss: 2.0349e-04\n",
      "10/10 - 0s - loss: 7.3192e-05\n",
      "10/10 - 0s - loss: 1.4660e-04\n",
      "10/10 - 0s - loss: 5.7045e-04\n",
      "10/10 - 0s - loss: 2.1281e-04\n",
      "10/10 - 0s - loss: 2.8162e-04\n",
      "10/10 - 0s - loss: 2.1751e-04\n",
      "10/10 - 0s - loss: 1.3022e-04\n",
      "10/10 - 0s - loss: 2.0065e-04\n",
      "10/10 - 0s - loss: 3.3907e-04\n",
      "10/10 - 0s - loss: 4.9412e-05\n",
      "10/10 - 0s - loss: 2.6308e-04\n",
      "10/10 - 0s - loss: 4.1629e-04\n",
      "10/10 - 0s - loss: 1.2200e-04\n",
      "10/10 - 0s - loss: 2.3010e-04\n",
      "10/10 - 0s - loss: 1.0199e-04\n",
      "10/10 - 0s - loss: 2.8677e-04\n",
      "10/10 - 0s - loss: 2.1755e-04\n",
      "10/10 - 0s - loss: 2.4592e-04\n",
      "10/10 - 0s - loss: 1.5389e-04\n",
      "10/10 - 0s - loss: 2.9528e-04\n",
      "10/10 - 0s - loss: 1.8053e-04\n",
      "10/10 - 0s - loss: 2.2507e-04\n",
      "10/10 - 0s - loss: 1.0558e-04\n",
      "10/10 - 0s - loss: 1.9617e-04\n",
      "10/10 - 0s - loss: 1.3055e-04\n",
      "10/10 - 0s - loss: 1.5163e-04\n",
      "10/10 - 0s - loss: 1.7056e-04\n",
      "10/10 - 0s - loss: 1.5769e-04\n",
      "10/10 - 0s - loss: 2.0782e-04\n",
      "10/10 - 0s - loss: 1.5815e-04\n",
      "10/10 - 0s - loss: 7.0734e-05\n",
      "10/10 - 0s - loss: 1.3559e-04\n",
      "10/10 - 0s - loss: 1.8092e-04\n",
      "10/10 - 0s - loss: 1.0262e-04\n",
      "10/10 - 0s - loss: 6.6589e-05\n",
      "10/10 - 0s - loss: 3.0544e-04\n",
      "10/10 - 0s - loss: 8.8221e-05\n",
      "10/10 - 0s - loss: 1.4006e-04\n",
      "10/10 - 0s - loss: 1.9253e-04\n",
      "10/10 - 0s - loss: 1.2849e-04\n",
      "10/10 - 0s - loss: 2.7220e-04\n",
      "10/10 - 0s - loss: 3.1320e-04\n",
      "10/10 - 0s - loss: 9.1685e-05\n",
      "10/10 - 0s - loss: 8.2965e-05\n",
      "10/10 - 0s - loss: 9.3685e-05\n",
      "10/10 - 0s - loss: 3.5987e-04\n",
      "10/10 - 0s - loss: 9.3187e-05\n",
      "10/10 - 0s - loss: 2.8055e-04\n",
      "10/10 - 0s - loss: 2.4431e-04\n",
      "10/10 - 0s - loss: 2.9000e-04\n",
      "10/10 - 0s - loss: 4.2421e-04\n",
      "10/10 - 0s - loss: 2.4152e-04\n",
      "10/10 - 0s - loss: 2.4994e-04\n",
      "10/10 - 0s - loss: 1.0871e-04\n",
      "10/10 - 0s - loss: 1.3115e-04\n",
      "10/10 - 0s - loss: 6.5147e-05\n",
      "10/10 - 0s - loss: 1.7694e-04\n",
      "10/10 - 0s - loss: 7.6363e-05\n",
      "10/10 - 0s - loss: 7.4234e-05\n",
      "10/10 - 0s - loss: 1.6885e-04\n",
      "10/10 - 0s - loss: 6.0074e-05\n",
      "10/10 - 0s - loss: 2.9359e-04\n",
      "10/10 - 0s - loss: 1.4908e-04\n",
      "10/10 - 0s - loss: 2.7033e-04\n",
      "10/10 - 0s - loss: 2.5716e-04\n",
      "10/10 - 0s - loss: 1.5766e-04\n",
      "10/10 - 0s - loss: 4.6431e-04\n",
      "10/10 - 0s - loss: 1.5332e-04\n",
      "10/10 - 0s - loss: 1.2179e-04\n",
      "10/10 - 0s - loss: 2.2762e-04\n",
      "10/10 - 0s - loss: 1.7235e-04\n",
      "10/10 - 0s - loss: 9.3294e-05\n",
      "10/10 - 0s - loss: 1.2019e-04\n",
      "10/10 - 0s - loss: 1.5642e-04\n",
      "10/10 - 0s - loss: 1.9264e-04\n",
      "10/10 - 0s - loss: 2.6647e-04\n",
      "10/10 - 0s - loss: 2.3995e-04\n",
      "10/10 - 0s - loss: 9.9298e-05\n",
      "10/10 - 0s - loss: 6.2255e-04\n",
      "10/10 - 0s - loss: 2.2306e-04\n",
      "10/10 - 0s - loss: 1.7148e-04\n",
      "10/10 - 0s - loss: 1.0478e-04\n",
      "10/10 - 0s - loss: 8.0137e-05\n",
      "10/10 - 0s - loss: 1.8149e-04\n",
      "10/10 - 0s - loss: 1.6175e-04\n",
      "10/10 - 0s - loss: 3.1867e-04\n",
      "10/10 - 0s - loss: 8.4729e-05\n",
      "10/10 - 0s - loss: 1.4455e-04\n",
      "10/10 - 0s - loss: 1.6544e-04\n",
      "10/10 - 0s - loss: 2.8416e-04\n",
      "10/10 - 0s - loss: 5.9094e-05\n",
      "10/10 - 0s - loss: 1.7834e-04\n",
      "10/10 - 0s - loss: 8.3631e-05\n",
      "10/10 - 0s - loss: 2.9457e-04\n",
      "10/10 - 0s - loss: 2.0565e-04\n",
      "10/10 - 0s - loss: 1.6699e-04\n",
      "10/10 - 0s - loss: 1.8933e-04\n",
      "10/10 - 0s - loss: 1.9969e-04\n",
      "10/10 - 0s - loss: 7.5553e-05\n",
      "10/10 - 0s - loss: 1.0248e-04\n",
      "10/10 - 0s - loss: 1.2174e-04\n",
      "10/10 - 0s - loss: 1.3943e-04\n",
      "10/10 - 0s - loss: 7.0262e-05\n",
      "10/10 - 0s - loss: 1.4731e-04\n",
      "10/10 - 0s - loss: 1.3653e-04\n",
      "10/10 - 0s - loss: 6.9285e-05\n",
      "10/10 - 0s - loss: 7.7679e-05\n",
      "10/10 - 0s - loss: 9.7437e-05\n",
      "10/10 - 0s - loss: 2.5176e-04\n",
      "10/10 - 0s - loss: 1.1344e-04\n",
      "10/10 - 0s - loss: 7.6546e-05\n",
      "10/10 - 0s - loss: 3.4690e-04\n",
      "10/10 - 0s - loss: 1.6582e-04\n",
      "10/10 - 0s - loss: 1.4758e-04\n",
      "10/10 - 0s - loss: 3.2080e-04\n",
      "10/10 - 0s - loss: 1.8659e-04\n",
      "10/10 - 0s - loss: 1.5410e-04\n",
      "10/10 - 0s - loss: 1.1694e-04\n",
      "10/10 - 0s - loss: 9.9995e-05\n",
      "10/10 - 0s - loss: 3.8542e-05\n",
      "10/10 - 0s - loss: 2.8909e-04\n",
      "10/10 - 0s - loss: 1.7392e-04\n",
      "10/10 - 0s - loss: 4.4258e-05\n",
      "10/10 - 0s - loss: 1.8694e-04\n",
      "10/10 - 0s - loss: 1.3638e-04\n",
      "10/10 - 0s - loss: 2.1569e-04\n",
      "10/10 - 0s - loss: 1.8736e-04\n",
      "10/10 - 0s - loss: 1.3086e-04\n",
      "10/10 - 0s - loss: 1.5157e-04\n",
      "10/10 - 0s - loss: 8.6152e-05\n",
      "10/10 - 0s - loss: 1.5024e-04\n",
      "10/10 - 0s - loss: 2.1517e-04\n",
      "10/10 - 0s - loss: 1.2706e-04\n",
      "10/10 - 0s - loss: 1.1506e-04\n",
      "10/10 - 0s - loss: 7.5254e-05\n",
      "10/10 - 0s - loss: 8.7476e-05\n",
      "10/10 - 0s - loss: 7.2049e-05\n",
      "10/10 - 0s - loss: 3.5367e-04\n",
      "10/10 - 0s - loss: 1.3778e-04\n",
      "10/10 - 0s - loss: 1.4323e-04\n",
      "10/10 - 0s - loss: 2.9509e-05\n",
      "10/10 - 0s - loss: 1.7056e-04\n",
      "10/10 - 0s - loss: 6.1207e-05\n",
      "10/10 - 0s - loss: 7.6457e-05\n",
      "10/10 - 0s - loss: 1.8449e-04\n",
      "10/10 - 0s - loss: 1.3200e-04\n",
      "10/10 - 0s - loss: 1.6528e-04\n",
      "10/10 - 0s - loss: 2.3124e-05\n",
      "10/10 - 0s - loss: 2.1759e-05\n",
      "10/10 - 0s - loss: 1.9100e-04\n",
      "10/10 - 0s - loss: 1.4472e-04\n",
      "10/10 - 0s - loss: 5.8488e-05\n",
      "10/10 - 0s - loss: 7.1873e-05\n",
      "10/10 - 0s - loss: 7.3409e-05\n",
      "10/10 - 0s - loss: 1.3813e-04\n",
      "10/10 - 0s - loss: 4.7279e-05\n",
      "10/10 - 0s - loss: 6.7897e-05\n",
      "10/10 - 0s - loss: 3.6498e-05\n",
      "10/10 - 0s - loss: 1.5536e-04\n",
      "10/10 - 0s - loss: 8.6246e-05\n",
      "10/10 - 0s - loss: 1.2979e-04\n",
      "10/10 - 0s - loss: 1.1878e-04\n",
      "10/10 - 0s - loss: 1.1417e-04\n",
      "Expected 0.2792924102809635 Predicted 0.2859535\n",
      "Expected 0.5007894985614483 Predicted 0.49072492\n",
      "Expected 0.17512819191676443 Predicted 0.1683774\n",
      "Expected 0.10653134137352804 Predicted 0.10718185\n",
      "Expected 0.47634884475549766 Predicted 0.46904835\n",
      "Expected 0.7053767094574427 Predicted 0.6990918\n",
      "Expected 0.09497046685509158 Predicted 0.10457186\n",
      "Expected 0.1912354248223107 Predicted 0.18557575\n",
      "Expected 0.1414973211967675 Predicted 0.13871588\n",
      "Expected 0.46580806411506936 Predicted 0.45768705\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "from numpy import array\n",
    "from pandas import concat\n",
    "from pandas import DataFrame\n",
    "# import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# generate a sequence of random values\n",
    "def generate_sequence(n_timesteps):\n",
    "\treturn [random() for _ in range(n_timesteps)]\n",
    "\n",
    "# generate data for the lstm\n",
    "def generate_data(n_timesteps):\n",
    "\t# generate sequence\n",
    "\tsequence = generate_sequence(n_timesteps)\n",
    "\tsequence = array(sequence)\n",
    "\t# create lag\n",
    "\tdf = DataFrame(sequence)\n",
    "\tdf = concat([df.shift(1), df], axis=1)\n",
    "\t# replace missing values with -1\n",
    "\tdf.fillna(-1, inplace=True)\n",
    "\tvalues = df.values\n",
    "\t# specify input and output data\n",
    "\tX, y = values, values[:, 1]\n",
    "\t# reshape\n",
    "\tX = X.reshape(len(X), 2, 1)\n",
    "\ty = y.reshape(len(y), 1)\n",
    "\treturn X, y\n",
    "\n",
    "n_timesteps = 10\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(5, input_shape=(2, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# fit model\n",
    "for i in range(500):\n",
    "\tX, y = generate_data(n_timesteps)\n",
    "\tmodel.fit(X, y, epochs=1, batch_size=1, verbose=2)\n",
    "# evaluate model on new data\n",
    "X, y = generate_data(n_timesteps)\n",
    "yhat = model.predict(X)\n",
    "for i in range(len(X)):\n",
    "\tprint('Expected', y[i,0], 'Predicted', yhat[i,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Маскирование отсутствующих значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Masking\n",
    "\n",
    "n_timesteps = 10\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=-1, input_shape=(2, 1)))\n",
    "model.add(LSTM(5))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# fit model\n",
    "for i in range(500):\n",
    "\tX, y = generate_data(n_timesteps)\n",
    "\tmodel.fit(X, y, epochs=1, batch_size=1, verbose=2)\n",
    "# evaluate model on new data\n",
    "X, y = generate_data(n_timesteps)\n",
    "yhat = model.predict(X)\n",
    "for i in range(len(X)):\n",
    "\tprint('Expected', y[i,0], 'Predicted', yhat[i,0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "073425a53dc9f576bec0cf2be1d8aa42855c2ffba435afe8e687f916b6655d7d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ML': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
